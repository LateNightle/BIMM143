---
title: "Class 08: Unsupervised Learning"
author: "Nathaniel Lightle (A16669288)"
format: pdf
---
## Outline
Today we will apply the machine learning methods we introduced in the last class on breast cancer biopsy data from fine needle aspiration (FNA). 

# Data input
The data is supplied in CSV format

## Exploratory Data analysis 

# Preparing the data

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names = 1)
head(wisc.df)
```

Removing the diagnosis

```{r}
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

Making the diagnosis into a factor

```{r}
diagnosis <- as.factor( wisc.df[,1])
head(diagnosis)
```

# Exploratory Data Analysis

> Q1. How many observations are in this dataset?

```{r}
dim(wisc.data)
``` 
There are 569 observations in this dataset.

> Q2. How many of the observations have a malignant diagnosis?

```{r}
NROW(diagnosis[diagnosis == "M"])
``` 
There are 212 malignant diagnosis


>Q3.How many variables/features in the data are suffixed with _mean?

```{r}
length(grep("_mean", colnames(wisc.data)))
``` 
There are 10 variables/features in the data suffixed with _mean

## 2. Principal Component Analysis

# Performing PCA

Checking to see if scaling is necessary

```{r}
colMeans(wisc.data)
apply(wisc.data, 2, sd)
```

We need to scale our input data before PCA as one of the columns are measured in terms of very different units with different means and different variances. I'm going to set `scale = TRUE` to fix this. 

```{r}
wisc.pr<-prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```

> Q4.From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.27% of the original variance is captured by PC1.

>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs are required to describe at least 70% of the original variance in the data.

>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs are required to describe at least 90% of the original variance in the data.

# Interpreting PCA Results

Plotting it!

```{r}
biplot(wisc.pr)
```

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

Every data point is negative in this plot. It's difficult to understand because it's so crowded and you can't make out any individual data point. 

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis)
```

The first plot is better because the two groups are more clearly defined. 

ggPlot time!

```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
library(ggplot2)

ggplot(df) + 
  aes (PC1, PC2, col=diagnosis) +
  geom_point()
```
# Variance Explained

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
pve <- pr.var / sum(pr.var)
plot(pve, xlab="Principal Component",
     ylab="Proportion of Variance Explained",
     ylim=c(0,1), type="o")
```

Optional Stuff
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
```{r}
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

# Communicating PCA Results

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation[,1]
```

The component of the loading vector for concave.points_mean is -0.26085376. 

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

```{r}
summary(wisc.pr)
```

5 PCs are required to explain 80% of the variance of the data.

## 3. Hierarchical Clustering

Scaling the data
```{r}
data.scaled <- scale(wisc.data)
```

Finding Euclidian Distance
```{r}
data.dist <- dist(data.scaled)
```

Creating a cluster
```{r}
wisc.hclust <- hclust(data.dist, method = "complete" )
plot(wisc.hclust)
```

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot(wisc.hclust)
abline(h=19, col = "red", lty=2)
```
At height 19 there are 4 clusters.

# Selecting number of clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)

table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

I can't. 4 clusters seems to be the best match.

# Using Different Methods

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

ward.D2 gives my favorite results because it's the easiest to look at. It's the best organized. 

## 5. Combining Methods

This approach will take not original data, but our PCA results and work with them. 

```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d, method = "ward.D2")
plot(wisc.pr.hclust)
```

Generating 2 cluster groups from this hclust object

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

Plotting it!

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=grps)
```

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```

Making the colors match up!

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
```

Using more PCs to try and more accurately predict diagnosis!

```{r}
x <- dist(wisc.pr$x[,1:7])
wisc.pr.hclust2 <- hclust(x, method="ward.D2")

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust2, k=2)
```

```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

The newly created model separates out the two diagnosis pretty well. Not quite as well as the other model, but still good. 


### END

